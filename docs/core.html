<!doctype html>
<html lang="en">
<head>
<meta charset="utf-8">
<meta name="viewport" content="width=device-width, initial-scale=1, minimum-scale=1" />
<meta name="generator" content="pdoc 0.9.2" />
<title>spych.core API documentation</title>
<meta name="description" content="" />
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/sanitize.min.css" integrity="sha256-PK9q560IAAa6WVRRh76LtCaI8pjTJ2z11v0miyNNjrs=" crossorigin>
<link rel="preload stylesheet" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/10up-sanitize.css/11.0.1/typography.min.css" integrity="sha256-7l/o7C8jubJiy74VsKTidCy1yBkRtiUGbVkYBylBqUg=" crossorigin>
<link rel="stylesheet preload" as="style" href="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/styles/github.min.css" crossorigin>
<style>:root{--highlight-color:#fe9}.flex{display:flex !important}body{line-height:1.5em}#content{padding:20px}#sidebar{padding:30px;overflow:hidden}#sidebar > *:last-child{margin-bottom:2cm}.http-server-breadcrumbs{font-size:130%;margin:0 0 15px 0}#footer{font-size:.75em;padding:5px 30px;border-top:1px solid #ddd;text-align:right}#footer p{margin:0 0 0 1em;display:inline-block}#footer p:last-child{margin-right:30px}h1,h2,h3,h4,h5{font-weight:300}h1{font-size:2.5em;line-height:1.1em}h2{font-size:1.75em;margin:1em 0 .50em 0}h3{font-size:1.4em;margin:25px 0 10px 0}h4{margin:0;font-size:105%}h1:target,h2:target,h3:target,h4:target,h5:target,h6:target{background:var(--highlight-color);padding:.2em 0}a{color:#058;text-decoration:none;transition:color .3s ease-in-out}a:hover{color:#e82}.title code{font-weight:bold}h2[id^="header-"]{margin-top:2em}.ident{color:#900}pre code{background:#f8f8f8;font-size:.8em;line-height:1.4em}code{background:#f2f2f1;padding:1px 4px;overflow-wrap:break-word}h1 code{background:transparent}pre{background:#f8f8f8;border:0;border-top:1px solid #ccc;border-bottom:1px solid #ccc;margin:1em 0;padding:1ex}#http-server-module-list{display:flex;flex-flow:column}#http-server-module-list div{display:flex}#http-server-module-list dt{min-width:10%}#http-server-module-list p{margin-top:0}.toc ul,#index{list-style-type:none;margin:0;padding:0}#index code{background:transparent}#index h3{border-bottom:1px solid #ddd}#index ul{padding:0}#index h4{margin-top:.6em;font-weight:bold}@media (min-width:200ex){#index .two-column{column-count:2}}@media (min-width:300ex){#index .two-column{column-count:3}}dl{margin-bottom:2em}dl dl:last-child{margin-bottom:4em}dd{margin:0 0 1em 3em}#header-classes + dl > dd{margin-bottom:3em}dd dd{margin-left:2em}dd p{margin:10px 0}.name{background:#eee;font-weight:bold;font-size:.85em;padding:5px 10px;display:inline-block;min-width:40%}.name:hover{background:#e0e0e0}dt:target .name{background:var(--highlight-color)}.name > span:first-child{white-space:nowrap}.name.class > span:nth-child(2){margin-left:.4em}.inherited{color:#999;border-left:5px solid #eee;padding-left:1em}.inheritance em{font-style:normal;font-weight:bold}.desc h2{font-weight:400;font-size:1.25em}.desc h3{font-size:1em}.desc dt code{background:inherit}.source summary,.git-link-div{color:#666;text-align:right;font-weight:400;font-size:.8em;text-transform:uppercase}.source summary > *{white-space:nowrap;cursor:pointer}.git-link{color:inherit;margin-left:1em}.source pre{max-height:500px;overflow:auto;margin:0}.source pre code{font-size:12px;overflow:visible}.hlist{list-style:none}.hlist li{display:inline}.hlist li:after{content:',\2002'}.hlist li:last-child:after{content:none}.hlist .hlist{display:inline;padding-left:1em}img{max-width:100%}td{padding:0 .5em}.admonition{padding:.1em .5em;margin-bottom:1em}.admonition-title{font-weight:bold}.admonition.note,.admonition.info,.admonition.important{background:#aef}.admonition.todo,.admonition.versionadded,.admonition.tip,.admonition.hint{background:#dfd}.admonition.warning,.admonition.versionchanged,.admonition.deprecated{background:#fd4}.admonition.error,.admonition.danger,.admonition.caution{background:lightpink}</style>
<style media="screen and (min-width: 700px)">@media screen and (min-width:700px){#sidebar{width:30%;height:100vh;overflow:auto;position:sticky;top:0}#content{width:70%;max-width:100ch;padding:3em 4em;border-left:1px solid #ddd}pre code{font-size:1em}.item .name{font-size:1em}main{display:flex;flex-direction:row-reverse;justify-content:flex-end}.toc ul ul,#index ul{padding-left:1.5em}.toc > ul > li{margin-top:.5em}}</style>
<style media="print">@media print{#sidebar h1{page-break-before:always}.source{display:none}}@media print{*{background:transparent !important;color:#000 !important;box-shadow:none !important;text-shadow:none !important}a[href]:after{content:" (" attr(href) ")";font-size:90%}a[href][title]:after{content:none}abbr[title]:after{content:" (" attr(title) ")"}.ir a:after,a[href^="javascript:"]:after,a[href^="#"]:after{content:""}pre,blockquote{border:1px solid #999;page-break-inside:avoid}thead{display:table-header-group}tr,img{page-break-inside:avoid}img{max-width:100% !important}@page{margin:0.5cm}p,h2,h3{orphans:3;widows:3}h1,h2,h3,h4,h5,h6{page-break-after:avoid}}</style>
<script defer src="https://cdnjs.cloudflare.com/ajax/libs/highlight.js/10.1.1/highlight.min.js" integrity="sha256-Uv3H6lx7dJmRfRvH8TH6kJD1TSK1aFcwgx+mdg3epi8=" crossorigin></script>
<script>window.addEventListener('DOMContentLoaded', () => hljs.initHighlighting())</script>
</head>
<body>
<main>
<article id="content">
<header>
<h1 class="title">Module <code>spych.core</code></h1>
</header>
<section id="section-intro">
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python"># Oringinal code modified from client.py
# https://github.com/mozilla/DeepSpeech/blob/master/native_client/python/client.py

from spych.utils import error
from deepspeech import Model, version

import numpy as np
import shlex, subprocess, sys, wave, json

class spych(error):
    def __init__(self, model_file, scorer_file=None):
        &#34;&#34;&#34;
        Initialize a spych class

        Required:

            - `model_file`:
                - Type: str
                - What: The location of your deepspeech model

        Optional:

            - `scorer_file`:
                - Type: str
                - What: The location of your deepspeech scorer
                - Default: None
        &#34;&#34;&#34;
        self.model_file=model_file
        self.scorer_file=scorer_file
        self.model = Model(self.model_file)
        if self.scorer_file:
            self.model.enableExternalScorer(self.scorer_file)
        self.desired_sample_rate=self.model.sampleRate()

    def execute_cmd(self, cmd, capture_output=True, check=True):
        &#34;&#34;&#34;
        Execute a subprocess cmd on the terminal / command line

        Required:

            - `cmd`:
                - Type: str
                - What: The command to execute
        &#34;&#34;&#34;
        try:
            output = subprocess.run(shlex.split(cmd), check=check, capture_output=capture_output)
        except subprocess.CalledProcessError as e:
            raise RuntimeError(f&#39;Execution of {cmd} returned non-zero status: {e.stderr}&#39;)
        except OSError as e:
            raise OSError(e.errno, f&#39;Execution of {cmd} returned OS Error: {e.strerror}&#39;)
        return output

    def parse_audio_sox(self, audio_file):
        &#34;&#34;&#34;
        Attempt auto formatting your audio file using SoX to match that of the DeepSpeech Model

        Required:

            - `audio_file`:
                - Type: str
                - What: The location of your target audio file to transcribe

        Returns:

            - `audio_buffer`:
                - Type: np.array
                - What: A 16-bit, mono raw audio signal at the appropriate sample rate serialized as a numpy array
                - Note: Exactly matches the DeepSpeech Model
        &#34;&#34;&#34;
        sox_cmd = f&#39;sox {shlex.quote(audio_file)} --type raw --bits 16 --channels 1 --rate {self.desired_sample_rate} --encoding signed-integer --endian little --compression 0.0 --no-dither - &#39;
        output = self.execute_cmd(sox_cmd)
        return np.frombuffer(output.stdout, np.int16)

    def parse_audio(self, audio_file):
        &#34;&#34;&#34;
        Helper function to parse your raw audio file to match audio structures for the DeepSpeech Model

        Required:

            - `audio_file`:
                - Type: str
                - What: The location of your target audio file to transcribe
                - Note: `.wav` files with the model specified sample rate are handled without external packages. Everything else gets converted using SoX (if possible)

        Returns:

            - `audio_buffer`:
                - Type: np.array
                - What: A 16-bit, mono raw audio signal at the appropriate sample rate serialized as a numpy array
                - Note: Exactly matches the DeepSpeech Model
        &#34;&#34;&#34;
        if &#34;.wav&#34; not in audio_file:
            self.warn(f&#34;Selected audio file is not in `.wav` format. Attempting SoX conversion.&#34;)
            return self.parse_audio_sox(audio_file=audio_file)
        with wave.open(audio_file, &#39;rb&#39;) as audio_raw:
            audio_sample_rate = audio_raw.getframerate()
            if audio_sample_rate != self.desired_sample_rate:
                self.warn(f&#34;Selected audio sample rate ({audio_sample_rate}) is different from the desired rate ({self.desired_sample_rate}). Attempting SoX conversion.&#34;)
                return self.parse_audio_sox(audio_file=audio_file)
            else:
                return np.frombuffer(audio_raw.readframes(audio_raw.getnframes()), np.int16)

    def record(self, output_audio_file=None, duration=3):
        &#34;&#34;&#34;
        Record an audio file for a set duration using SoX

        Optional:

            - `output_audio_file`:
                - Type: str
                - What: The location to output the collected recording
                - Default: None
                - Note: Must be a `.wav` file
                - Note: If specified, the audio file location is returned
                - Note: If not specified or None, an audio buffer is returned

            - `duration`:
                - Type: int
                - What: The duration of time to record in seconds
                - Default: 3

        Returns:

            - `audio_file`:
                - Type: str
                - What: The provided `output_audio_file` given at the invokation of this function
                - Note: Returned only if `output_audio_file` is specified

            - OR

            - `audio_buffer`:
                - Type: np.array
                - What: A 16-bit, mono raw audio signal at the appropriate sample rate serialized as a numpy array
                - Note: Exactly matches the DeepSpeech Model
                - Note: Returned only if `output_audio_file` is not specified

        &#34;&#34;&#34;
        if output_audio_file:
            sox_cmd = f&#39;sox -d --channels 1 --rate {self.desired_sample_rate} --no-dither {shlex.quote(output_audio_file)} trim 0 {duration}&#39;
            output=self.execute_cmd(sox_cmd)
            return output_audio_file
        else:
            sox_cmd = f&#39;sox -d --type raw --bits 16 --channels 1 --rate {self.desired_sample_rate} --encoding signed-integer --endian little --compression 0.0 --no-dither - trim 0 {duration}&#39;
            output = self.execute_cmd(sox_cmd)
            return np.frombuffer(output.stdout, np.int16)

    def play(self, audio_file):
        &#34;&#34;&#34;
        Play an audio file using SoX

        Required:

            - `audio_file`:
                - Type: str
                - What: The location of your target audio file to transcribe
        &#34;&#34;&#34;
        sox_cmd = f&#39;sox {shlex.quote(audio_file)} -d&#39;
        self.execute_cmd(sox_cmd)

    def get_word_timings(self, transcript):
        &#34;&#34;&#34;
        Helper function to parse word timings and duration from a transcription metadata object

        Required:

            - `transcript`:
                - Type: CandidateTranscript (from deepspeech)
                - What: The candidate transcript to parse

        Returns:

            - `timings`:
                - Type: dict
                - What: A dictionary of the `start_time`, `end_time` and `duration` for each word in this transcript where those values are provided in seconds
        &#34;&#34;&#34;
        if len(transcript.tokens)==0:
            return []
        word_data=[]
        word_tokens=[]
        for token in transcript.tokens:
            word_tokens.append(token)
            if token.text==&#34; &#34;:
                word_data.append(word_tokens)
                word_tokens=[]
        word_data.append(word_tokens)
        output=[]
        for word_tokens in word_data:
            try:
                start=round(word_tokens[0].start_time,3)
                end=round(word_tokens[-1].start_time,3)
                output.append({
                    &#39;start&#39;:start,
                    &#39;end&#39;:end,
                    &#39;duration&#39;:round(end-start,3)
                })
            except:
                pass
        return output

    def get_transcript_dict(self, transcript, return_text=True, return_confidence=False, return_words=False, return_word_timings=False, return_meta=False):
        &#34;&#34;&#34;
        Helper function to parse a clean dictionary from a transcription metadata object

        Required:

            - `transcript`:
                - Type: CandidateTranscript (from deepspeech)
                - What: The candidate transcript to parse

        Optional:

            - `return_text`:
                - Type: bool
                - What: Flag to indicate if the predicted text should be returned
                - Default: True
            - `return_confidence`:
                - Type: bool
                - What: Flag to indicate if the confidence level for this text should be returned
                - Default: False
            - `return_words`:
                - Type: bool
                - What: Flag to indicate if a words list (from the predicted text) should be returned
                - Default: False
            - `return_word_timings`:
                - Type: bool
                - What: Flag to indicate if the predicted timings (start, end and duration) for each word should be returned
                - Default: False
            - `return_meta`:
                - Type: bool
                - What: Flag to indicate if the transcript metadata object from DeepSpeech should be returned
                - Default: False

        Returns:

            - `transcript_dictionary`:
                - Type: dict
                - What: Dictionary of serialized transcription items specified by optional inputs
        &#34;&#34;&#34;
        string=&#39;&#39;.join(i.text for i in transcript.tokens)
        output={}
        if return_text:
            output[&#39;text&#39;]=string
        if return_confidence:
            output[&#39;confidence&#39;]=transcript.confidence
        if return_words:
            output[&#39;words&#39;]=string.split(&#34; &#34;)
        if return_word_timings:
            output[&#39;words_timings&#39;]=self.get_word_timings(transcript)
        if return_meta:
            output[&#39;meta&#39;]=transcript
        return output

    def stt(self, audio_buffer=None, audio_file=None, ):
        &#34;&#34;&#34;
        Compute speech-to-text transcription for a provided audio file

        Required:

            - `audio_buffer`:
                - Type: np.array
                - What: A 16-bit, mono raw audio signal at the appropriate sample rate serialized as a numpy array
                - Note: Exactly matches the DeepSpeech Model

            - OR

            - `audio_file`:
                - Type: str
                - What: The location of your target audio file to transcribe

        Returns:

            - `text`:
                - Type: str
                - What: The transcribed text
        &#34;&#34;&#34;
        if audio_file:
            audio_buffer = self.parse_audio(audio_file)
        if audio_buffer is None:
            self.exception(&#34;You must specify a valid audio_file or audio_buffer&#34;)
        return self.model.stt(audio_buffer)

    def stt_expanded(self, audio_file=None, audio_buffer=None, num_candidates=1, **kwargs):
        &#34;&#34;&#34;
        Compute speech-to-text with extra data for N predicted candidates given an audio file

        Required:

            - `audio_buffer`:
                - Type: np.array
                - What: A 16-bit, mono raw audio signal at the appropriate sample rate serialized as a numpy array
                - Note: Exactly matches the DeepSpeech Model

            - OR

            - `audio_file`:
                - Type: str
                - What: The location of your target audio file to transcribe

        Optional:

            - `num_candidates`:
                - Type: int
                - What: The number of potential transcript candidates to return
                - Default: 1
                - Note: The most confident/likely result appears first
            - `return_text`:
                - Type: bool
                - What: Flag to indicate if the predicted text should be returned
                - Default: True
            - `return_confidence`:
                - Type: bool
                - What: Flag to indicate if the confidence level for this text should be returned
                - Default: False
            - `return_words`:
                - Type: bool
                - What: Flag to indicate if a words list (from the predicted text) should be returned
                - Default: False
            - `return_word_timings`:
                - Type: bool
                - What: Flag to indicate if the predicted timings (start, end and duration) for each word should be returned
                - Default: False
            - `return_meta`:
                - Type: bool
                - What: Flag to indicate if the transcript metadata object from DeepSpeech should be returned
                - Default: False

        Returns:

            - `transcriptions`:
                - Type: list of dictionaries
                - What: A list of dictionaries with various transcription output data
        &#34;&#34;&#34;
        if audio_file:
            audio_buffer = self.parse_audio(audio_file)
        if audio_buffer is None:
            self.exception(&#34;You must specify a valid audio_file or audio_buffer&#34;)
        output_meta=self.model.sttWithMetadata(audio_buffer, num_candidates)
        return [self.get_transcript_dict(transcript, **kwargs) for transcript in output_meta.transcripts]

    def stt_list(self, audio_file=None, audio_buffer=None, num_candidates=3):
        &#34;&#34;&#34;
        Compute speech-to-text with extra data for N predicted candidates given an audio file

        Required:

            - `audio_buffer`:
                - Type: np.array
                - What: A 16-bit, mono raw audio signal at the appropriate sample rate serialized as a numpy array
                - Note: Exactly matches the DeepSpeech Model

            - OR

            - `audio_file`:
                - Type: str
                - What: The location of your target audio file to transcribe

        Optional:

            - `num_candidates`:
                - Type: int
                - What: The number of potential transcript candidates to return
                - Default: 1
                - Note: The most confident/likely result appears first

        Returns:

            - `transcriptions`:
                - Type: list of strs
                - What: A list of potential transcriptions in decending order of confidence
        &#34;&#34;&#34;
        expanded_list=self.stt_expanded(audio_file=audio_file, audio_buffer=audio_buffer, num_candidates=num_candidates, return_text=True)
        return [i[&#39;text&#39;] for i in expanded_list]</code></pre>
</details>
</section>
<section>
</section>
<section>
</section>
<section>
</section>
<section>
<h2 class="section-title" id="header-classes">Classes</h2>
<dl>
<dt id="spych.core.spych"><code class="flex name class">
<span>class <span class="ident">spych</span></span>
<span>(</span><span>model_file, scorer_file=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Initialize a spych class</p>
<h2 id="required">Required</h2>
<ul>
<li><code>model_file</code>:<ul>
<li>Type: str</li>
<li>What: The location of your deepspeech model</li>
</ul>
</li>
</ul>
<h2 id="optional">Optional</h2>
<ul>
<li><code>scorer_file</code>:<ul>
<li>Type: str</li>
<li>What: The location of your deepspeech scorer</li>
<li>Default: None</li>
</ul>
</li>
</ul></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">class spych(error):
    def __init__(self, model_file, scorer_file=None):
        &#34;&#34;&#34;
        Initialize a spych class

        Required:

            - `model_file`:
                - Type: str
                - What: The location of your deepspeech model

        Optional:

            - `scorer_file`:
                - Type: str
                - What: The location of your deepspeech scorer
                - Default: None
        &#34;&#34;&#34;
        self.model_file=model_file
        self.scorer_file=scorer_file
        self.model = Model(self.model_file)
        if self.scorer_file:
            self.model.enableExternalScorer(self.scorer_file)
        self.desired_sample_rate=self.model.sampleRate()

    def execute_cmd(self, cmd, capture_output=True, check=True):
        &#34;&#34;&#34;
        Execute a subprocess cmd on the terminal / command line

        Required:

            - `cmd`:
                - Type: str
                - What: The command to execute
        &#34;&#34;&#34;
        try:
            output = subprocess.run(shlex.split(cmd), check=check, capture_output=capture_output)
        except subprocess.CalledProcessError as e:
            raise RuntimeError(f&#39;Execution of {cmd} returned non-zero status: {e.stderr}&#39;)
        except OSError as e:
            raise OSError(e.errno, f&#39;Execution of {cmd} returned OS Error: {e.strerror}&#39;)
        return output

    def parse_audio_sox(self, audio_file):
        &#34;&#34;&#34;
        Attempt auto formatting your audio file using SoX to match that of the DeepSpeech Model

        Required:

            - `audio_file`:
                - Type: str
                - What: The location of your target audio file to transcribe

        Returns:

            - `audio_buffer`:
                - Type: np.array
                - What: A 16-bit, mono raw audio signal at the appropriate sample rate serialized as a numpy array
                - Note: Exactly matches the DeepSpeech Model
        &#34;&#34;&#34;
        sox_cmd = f&#39;sox {shlex.quote(audio_file)} --type raw --bits 16 --channels 1 --rate {self.desired_sample_rate} --encoding signed-integer --endian little --compression 0.0 --no-dither - &#39;
        output = self.execute_cmd(sox_cmd)
        return np.frombuffer(output.stdout, np.int16)

    def parse_audio(self, audio_file):
        &#34;&#34;&#34;
        Helper function to parse your raw audio file to match audio structures for the DeepSpeech Model

        Required:

            - `audio_file`:
                - Type: str
                - What: The location of your target audio file to transcribe
                - Note: `.wav` files with the model specified sample rate are handled without external packages. Everything else gets converted using SoX (if possible)

        Returns:

            - `audio_buffer`:
                - Type: np.array
                - What: A 16-bit, mono raw audio signal at the appropriate sample rate serialized as a numpy array
                - Note: Exactly matches the DeepSpeech Model
        &#34;&#34;&#34;
        if &#34;.wav&#34; not in audio_file:
            self.warn(f&#34;Selected audio file is not in `.wav` format. Attempting SoX conversion.&#34;)
            return self.parse_audio_sox(audio_file=audio_file)
        with wave.open(audio_file, &#39;rb&#39;) as audio_raw:
            audio_sample_rate = audio_raw.getframerate()
            if audio_sample_rate != self.desired_sample_rate:
                self.warn(f&#34;Selected audio sample rate ({audio_sample_rate}) is different from the desired rate ({self.desired_sample_rate}). Attempting SoX conversion.&#34;)
                return self.parse_audio_sox(audio_file=audio_file)
            else:
                return np.frombuffer(audio_raw.readframes(audio_raw.getnframes()), np.int16)

    def record(self, output_audio_file=None, duration=3):
        &#34;&#34;&#34;
        Record an audio file for a set duration using SoX

        Optional:

            - `output_audio_file`:
                - Type: str
                - What: The location to output the collected recording
                - Default: None
                - Note: Must be a `.wav` file
                - Note: If specified, the audio file location is returned
                - Note: If not specified or None, an audio buffer is returned

            - `duration`:
                - Type: int
                - What: The duration of time to record in seconds
                - Default: 3

        Returns:

            - `audio_file`:
                - Type: str
                - What: The provided `output_audio_file` given at the invokation of this function
                - Note: Returned only if `output_audio_file` is specified

            - OR

            - `audio_buffer`:
                - Type: np.array
                - What: A 16-bit, mono raw audio signal at the appropriate sample rate serialized as a numpy array
                - Note: Exactly matches the DeepSpeech Model
                - Note: Returned only if `output_audio_file` is not specified

        &#34;&#34;&#34;
        if output_audio_file:
            sox_cmd = f&#39;sox -d --channels 1 --rate {self.desired_sample_rate} --no-dither {shlex.quote(output_audio_file)} trim 0 {duration}&#39;
            output=self.execute_cmd(sox_cmd)
            return output_audio_file
        else:
            sox_cmd = f&#39;sox -d --type raw --bits 16 --channels 1 --rate {self.desired_sample_rate} --encoding signed-integer --endian little --compression 0.0 --no-dither - trim 0 {duration}&#39;
            output = self.execute_cmd(sox_cmd)
            return np.frombuffer(output.stdout, np.int16)

    def play(self, audio_file):
        &#34;&#34;&#34;
        Play an audio file using SoX

        Required:

            - `audio_file`:
                - Type: str
                - What: The location of your target audio file to transcribe
        &#34;&#34;&#34;
        sox_cmd = f&#39;sox {shlex.quote(audio_file)} -d&#39;
        self.execute_cmd(sox_cmd)

    def get_word_timings(self, transcript):
        &#34;&#34;&#34;
        Helper function to parse word timings and duration from a transcription metadata object

        Required:

            - `transcript`:
                - Type: CandidateTranscript (from deepspeech)
                - What: The candidate transcript to parse

        Returns:

            - `timings`:
                - Type: dict
                - What: A dictionary of the `start_time`, `end_time` and `duration` for each word in this transcript where those values are provided in seconds
        &#34;&#34;&#34;
        if len(transcript.tokens)==0:
            return []
        word_data=[]
        word_tokens=[]
        for token in transcript.tokens:
            word_tokens.append(token)
            if token.text==&#34; &#34;:
                word_data.append(word_tokens)
                word_tokens=[]
        word_data.append(word_tokens)
        output=[]
        for word_tokens in word_data:
            try:
                start=round(word_tokens[0].start_time,3)
                end=round(word_tokens[-1].start_time,3)
                output.append({
                    &#39;start&#39;:start,
                    &#39;end&#39;:end,
                    &#39;duration&#39;:round(end-start,3)
                })
            except:
                pass
        return output

    def get_transcript_dict(self, transcript, return_text=True, return_confidence=False, return_words=False, return_word_timings=False, return_meta=False):
        &#34;&#34;&#34;
        Helper function to parse a clean dictionary from a transcription metadata object

        Required:

            - `transcript`:
                - Type: CandidateTranscript (from deepspeech)
                - What: The candidate transcript to parse

        Optional:

            - `return_text`:
                - Type: bool
                - What: Flag to indicate if the predicted text should be returned
                - Default: True
            - `return_confidence`:
                - Type: bool
                - What: Flag to indicate if the confidence level for this text should be returned
                - Default: False
            - `return_words`:
                - Type: bool
                - What: Flag to indicate if a words list (from the predicted text) should be returned
                - Default: False
            - `return_word_timings`:
                - Type: bool
                - What: Flag to indicate if the predicted timings (start, end and duration) for each word should be returned
                - Default: False
            - `return_meta`:
                - Type: bool
                - What: Flag to indicate if the transcript metadata object from DeepSpeech should be returned
                - Default: False

        Returns:

            - `transcript_dictionary`:
                - Type: dict
                - What: Dictionary of serialized transcription items specified by optional inputs
        &#34;&#34;&#34;
        string=&#39;&#39;.join(i.text for i in transcript.tokens)
        output={}
        if return_text:
            output[&#39;text&#39;]=string
        if return_confidence:
            output[&#39;confidence&#39;]=transcript.confidence
        if return_words:
            output[&#39;words&#39;]=string.split(&#34; &#34;)
        if return_word_timings:
            output[&#39;words_timings&#39;]=self.get_word_timings(transcript)
        if return_meta:
            output[&#39;meta&#39;]=transcript
        return output

    def stt(self, audio_buffer=None, audio_file=None, ):
        &#34;&#34;&#34;
        Compute speech-to-text transcription for a provided audio file

        Required:

            - `audio_buffer`:
                - Type: np.array
                - What: A 16-bit, mono raw audio signal at the appropriate sample rate serialized as a numpy array
                - Note: Exactly matches the DeepSpeech Model

            - OR

            - `audio_file`:
                - Type: str
                - What: The location of your target audio file to transcribe

        Returns:

            - `text`:
                - Type: str
                - What: The transcribed text
        &#34;&#34;&#34;
        if audio_file:
            audio_buffer = self.parse_audio(audio_file)
        if audio_buffer is None:
            self.exception(&#34;You must specify a valid audio_file or audio_buffer&#34;)
        return self.model.stt(audio_buffer)

    def stt_expanded(self, audio_file=None, audio_buffer=None, num_candidates=1, **kwargs):
        &#34;&#34;&#34;
        Compute speech-to-text with extra data for N predicted candidates given an audio file

        Required:

            - `audio_buffer`:
                - Type: np.array
                - What: A 16-bit, mono raw audio signal at the appropriate sample rate serialized as a numpy array
                - Note: Exactly matches the DeepSpeech Model

            - OR

            - `audio_file`:
                - Type: str
                - What: The location of your target audio file to transcribe

        Optional:

            - `num_candidates`:
                - Type: int
                - What: The number of potential transcript candidates to return
                - Default: 1
                - Note: The most confident/likely result appears first
            - `return_text`:
                - Type: bool
                - What: Flag to indicate if the predicted text should be returned
                - Default: True
            - `return_confidence`:
                - Type: bool
                - What: Flag to indicate if the confidence level for this text should be returned
                - Default: False
            - `return_words`:
                - Type: bool
                - What: Flag to indicate if a words list (from the predicted text) should be returned
                - Default: False
            - `return_word_timings`:
                - Type: bool
                - What: Flag to indicate if the predicted timings (start, end and duration) for each word should be returned
                - Default: False
            - `return_meta`:
                - Type: bool
                - What: Flag to indicate if the transcript metadata object from DeepSpeech should be returned
                - Default: False

        Returns:

            - `transcriptions`:
                - Type: list of dictionaries
                - What: A list of dictionaries with various transcription output data
        &#34;&#34;&#34;
        if audio_file:
            audio_buffer = self.parse_audio(audio_file)
        if audio_buffer is None:
            self.exception(&#34;You must specify a valid audio_file or audio_buffer&#34;)
        output_meta=self.model.sttWithMetadata(audio_buffer, num_candidates)
        return [self.get_transcript_dict(transcript, **kwargs) for transcript in output_meta.transcripts]

    def stt_list(self, audio_file=None, audio_buffer=None, num_candidates=3):
        &#34;&#34;&#34;
        Compute speech-to-text with extra data for N predicted candidates given an audio file

        Required:

            - `audio_buffer`:
                - Type: np.array
                - What: A 16-bit, mono raw audio signal at the appropriate sample rate serialized as a numpy array
                - Note: Exactly matches the DeepSpeech Model

            - OR

            - `audio_file`:
                - Type: str
                - What: The location of your target audio file to transcribe

        Optional:

            - `num_candidates`:
                - Type: int
                - What: The number of potential transcript candidates to return
                - Default: 1
                - Note: The most confident/likely result appears first

        Returns:

            - `transcriptions`:
                - Type: list of strs
                - What: A list of potential transcriptions in decending order of confidence
        &#34;&#34;&#34;
        expanded_list=self.stt_expanded(audio_file=audio_file, audio_buffer=audio_buffer, num_candidates=num_candidates, return_text=True)
        return [i[&#39;text&#39;] for i in expanded_list]</code></pre>
</details>
<h3>Ancestors</h3>
<ul class="hlist">
<li><a title="spych.utils.error" href="utils.html#spych.utils.error">error</a></li>
</ul>
<h3>Methods</h3>
<dl>
<dt id="spych.core.spych.execute_cmd"><code class="name flex">
<span>def <span class="ident">execute_cmd</span></span>(<span>self, cmd, capture_output=True, check=True)</span>
</code></dt>
<dd>
<div class="desc"><p>Execute a subprocess cmd on the terminal / command line</p>
<h2 id="required">Required</h2>
<ul>
<li><code>cmd</code>:<ul>
<li>Type: str</li>
<li>What: The command to execute</li>
</ul>
</li>
</ul></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def execute_cmd(self, cmd, capture_output=True, check=True):
    &#34;&#34;&#34;
    Execute a subprocess cmd on the terminal / command line

    Required:

        - `cmd`:
            - Type: str
            - What: The command to execute
    &#34;&#34;&#34;
    try:
        output = subprocess.run(shlex.split(cmd), check=check, capture_output=capture_output)
    except subprocess.CalledProcessError as e:
        raise RuntimeError(f&#39;Execution of {cmd} returned non-zero status: {e.stderr}&#39;)
    except OSError as e:
        raise OSError(e.errno, f&#39;Execution of {cmd} returned OS Error: {e.strerror}&#39;)
    return output</code></pre>
</details>
</dd>
<dt id="spych.core.spych.get_transcript_dict"><code class="name flex">
<span>def <span class="ident">get_transcript_dict</span></span>(<span>self, transcript, return_text=True, return_confidence=False, return_words=False, return_word_timings=False, return_meta=False)</span>
</code></dt>
<dd>
<div class="desc"><p>Helper function to parse a clean dictionary from a transcription metadata object</p>
<h2 id="required">Required</h2>
<ul>
<li><code>transcript</code>:<ul>
<li>Type: CandidateTranscript (from deepspeech)</li>
<li>What: The candidate transcript to parse</li>
</ul>
</li>
</ul>
<h2 id="optional">Optional</h2>
<ul>
<li><code>return_text</code>:<ul>
<li>Type: bool</li>
<li>What: Flag to indicate if the predicted text should be returned</li>
<li>Default: True</li>
</ul>
</li>
<li><code>return_confidence</code>:<ul>
<li>Type: bool</li>
<li>What: Flag to indicate if the confidence level for this text should be returned</li>
<li>Default: False</li>
</ul>
</li>
<li><code>return_words</code>:<ul>
<li>Type: bool</li>
<li>What: Flag to indicate if a words list (from the predicted text) should be returned</li>
<li>Default: False</li>
</ul>
</li>
<li><code>return_word_timings</code>:<ul>
<li>Type: bool</li>
<li>What: Flag to indicate if the predicted timings (start, end and duration) for each word should be returned</li>
<li>Default: False</li>
</ul>
</li>
<li><code>return_meta</code>:<ul>
<li>Type: bool</li>
<li>What: Flag to indicate if the transcript metadata object from DeepSpeech should be returned</li>
<li>Default: False</li>
</ul>
</li>
</ul>
<h2 id="returns">Returns</h2>
<ul>
<li><code>transcript_dictionary</code>:<ul>
<li>Type: dict</li>
<li>What: Dictionary of serialized transcription items specified by optional inputs</li>
</ul>
</li>
</ul></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_transcript_dict(self, transcript, return_text=True, return_confidence=False, return_words=False, return_word_timings=False, return_meta=False):
    &#34;&#34;&#34;
    Helper function to parse a clean dictionary from a transcription metadata object

    Required:

        - `transcript`:
            - Type: CandidateTranscript (from deepspeech)
            - What: The candidate transcript to parse

    Optional:

        - `return_text`:
            - Type: bool
            - What: Flag to indicate if the predicted text should be returned
            - Default: True
        - `return_confidence`:
            - Type: bool
            - What: Flag to indicate if the confidence level for this text should be returned
            - Default: False
        - `return_words`:
            - Type: bool
            - What: Flag to indicate if a words list (from the predicted text) should be returned
            - Default: False
        - `return_word_timings`:
            - Type: bool
            - What: Flag to indicate if the predicted timings (start, end and duration) for each word should be returned
            - Default: False
        - `return_meta`:
            - Type: bool
            - What: Flag to indicate if the transcript metadata object from DeepSpeech should be returned
            - Default: False

    Returns:

        - `transcript_dictionary`:
            - Type: dict
            - What: Dictionary of serialized transcription items specified by optional inputs
    &#34;&#34;&#34;
    string=&#39;&#39;.join(i.text for i in transcript.tokens)
    output={}
    if return_text:
        output[&#39;text&#39;]=string
    if return_confidence:
        output[&#39;confidence&#39;]=transcript.confidence
    if return_words:
        output[&#39;words&#39;]=string.split(&#34; &#34;)
    if return_word_timings:
        output[&#39;words_timings&#39;]=self.get_word_timings(transcript)
    if return_meta:
        output[&#39;meta&#39;]=transcript
    return output</code></pre>
</details>
</dd>
<dt id="spych.core.spych.get_word_timings"><code class="name flex">
<span>def <span class="ident">get_word_timings</span></span>(<span>self, transcript)</span>
</code></dt>
<dd>
<div class="desc"><p>Helper function to parse word timings and duration from a transcription metadata object</p>
<h2 id="required">Required</h2>
<ul>
<li><code>transcript</code>:<ul>
<li>Type: CandidateTranscript (from deepspeech)</li>
<li>What: The candidate transcript to parse</li>
</ul>
</li>
</ul>
<h2 id="returns">Returns</h2>
<ul>
<li><code>timings</code>:<ul>
<li>Type: dict</li>
<li>What: A dictionary of the <code>start_time</code>, <code>end_time</code> and <code>duration</code> for each word in this transcript where those values are provided in seconds</li>
</ul>
</li>
</ul></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def get_word_timings(self, transcript):
    &#34;&#34;&#34;
    Helper function to parse word timings and duration from a transcription metadata object

    Required:

        - `transcript`:
            - Type: CandidateTranscript (from deepspeech)
            - What: The candidate transcript to parse

    Returns:

        - `timings`:
            - Type: dict
            - What: A dictionary of the `start_time`, `end_time` and `duration` for each word in this transcript where those values are provided in seconds
    &#34;&#34;&#34;
    if len(transcript.tokens)==0:
        return []
    word_data=[]
    word_tokens=[]
    for token in transcript.tokens:
        word_tokens.append(token)
        if token.text==&#34; &#34;:
            word_data.append(word_tokens)
            word_tokens=[]
    word_data.append(word_tokens)
    output=[]
    for word_tokens in word_data:
        try:
            start=round(word_tokens[0].start_time,3)
            end=round(word_tokens[-1].start_time,3)
            output.append({
                &#39;start&#39;:start,
                &#39;end&#39;:end,
                &#39;duration&#39;:round(end-start,3)
            })
        except:
            pass
    return output</code></pre>
</details>
</dd>
<dt id="spych.core.spych.parse_audio"><code class="name flex">
<span>def <span class="ident">parse_audio</span></span>(<span>self, audio_file)</span>
</code></dt>
<dd>
<div class="desc"><p>Helper function to parse your raw audio file to match audio structures for the DeepSpeech Model</p>
<h2 id="required">Required</h2>
<ul>
<li><code>audio_file</code>:<ul>
<li>Type: str</li>
<li>What: The location of your target audio file to transcribe</li>
<li>Note: <code>.wav</code> files with the model specified sample rate are handled without external packages. Everything else gets converted using SoX (if possible)</li>
</ul>
</li>
</ul>
<h2 id="returns">Returns</h2>
<ul>
<li><code>audio_buffer</code>:<ul>
<li>Type: np.array</li>
<li>What: A 16-bit, mono raw audio signal at the appropriate sample rate serialized as a numpy array</li>
<li>Note: Exactly matches the DeepSpeech Model</li>
</ul>
</li>
</ul></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def parse_audio(self, audio_file):
    &#34;&#34;&#34;
    Helper function to parse your raw audio file to match audio structures for the DeepSpeech Model

    Required:

        - `audio_file`:
            - Type: str
            - What: The location of your target audio file to transcribe
            - Note: `.wav` files with the model specified sample rate are handled without external packages. Everything else gets converted using SoX (if possible)

    Returns:

        - `audio_buffer`:
            - Type: np.array
            - What: A 16-bit, mono raw audio signal at the appropriate sample rate serialized as a numpy array
            - Note: Exactly matches the DeepSpeech Model
    &#34;&#34;&#34;
    if &#34;.wav&#34; not in audio_file:
        self.warn(f&#34;Selected audio file is not in `.wav` format. Attempting SoX conversion.&#34;)
        return self.parse_audio_sox(audio_file=audio_file)
    with wave.open(audio_file, &#39;rb&#39;) as audio_raw:
        audio_sample_rate = audio_raw.getframerate()
        if audio_sample_rate != self.desired_sample_rate:
            self.warn(f&#34;Selected audio sample rate ({audio_sample_rate}) is different from the desired rate ({self.desired_sample_rate}). Attempting SoX conversion.&#34;)
            return self.parse_audio_sox(audio_file=audio_file)
        else:
            return np.frombuffer(audio_raw.readframes(audio_raw.getnframes()), np.int16)</code></pre>
</details>
</dd>
<dt id="spych.core.spych.parse_audio_sox"><code class="name flex">
<span>def <span class="ident">parse_audio_sox</span></span>(<span>self, audio_file)</span>
</code></dt>
<dd>
<div class="desc"><p>Attempt auto formatting your audio file using SoX to match that of the DeepSpeech Model</p>
<h2 id="required">Required</h2>
<ul>
<li><code>audio_file</code>:<ul>
<li>Type: str</li>
<li>What: The location of your target audio file to transcribe</li>
</ul>
</li>
</ul>
<h2 id="returns">Returns</h2>
<ul>
<li><code>audio_buffer</code>:<ul>
<li>Type: np.array</li>
<li>What: A 16-bit, mono raw audio signal at the appropriate sample rate serialized as a numpy array</li>
<li>Note: Exactly matches the DeepSpeech Model</li>
</ul>
</li>
</ul></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def parse_audio_sox(self, audio_file):
    &#34;&#34;&#34;
    Attempt auto formatting your audio file using SoX to match that of the DeepSpeech Model

    Required:

        - `audio_file`:
            - Type: str
            - What: The location of your target audio file to transcribe

    Returns:

        - `audio_buffer`:
            - Type: np.array
            - What: A 16-bit, mono raw audio signal at the appropriate sample rate serialized as a numpy array
            - Note: Exactly matches the DeepSpeech Model
    &#34;&#34;&#34;
    sox_cmd = f&#39;sox {shlex.quote(audio_file)} --type raw --bits 16 --channels 1 --rate {self.desired_sample_rate} --encoding signed-integer --endian little --compression 0.0 --no-dither - &#39;
    output = self.execute_cmd(sox_cmd)
    return np.frombuffer(output.stdout, np.int16)</code></pre>
</details>
</dd>
<dt id="spych.core.spych.play"><code class="name flex">
<span>def <span class="ident">play</span></span>(<span>self, audio_file)</span>
</code></dt>
<dd>
<div class="desc"><p>Play an audio file using SoX</p>
<h2 id="required">Required</h2>
<ul>
<li><code>audio_file</code>:<ul>
<li>Type: str</li>
<li>What: The location of your target audio file to transcribe</li>
</ul>
</li>
</ul></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def play(self, audio_file):
    &#34;&#34;&#34;
    Play an audio file using SoX

    Required:

        - `audio_file`:
            - Type: str
            - What: The location of your target audio file to transcribe
    &#34;&#34;&#34;
    sox_cmd = f&#39;sox {shlex.quote(audio_file)} -d&#39;
    self.execute_cmd(sox_cmd)</code></pre>
</details>
</dd>
<dt id="spych.core.spych.record"><code class="name flex">
<span>def <span class="ident">record</span></span>(<span>self, output_audio_file=None, duration=3)</span>
</code></dt>
<dd>
<div class="desc"><p>Record an audio file for a set duration using SoX</p>
<h2 id="optional">Optional</h2>
<ul>
<li>
<p><code>output_audio_file</code>:</p>
<ul>
<li>Type: str</li>
<li>What: The location to output the collected recording</li>
<li>Default: None</li>
<li>Note: Must be a <code>.wav</code> file</li>
<li>Note: If specified, the audio file location is returned</li>
<li>Note: If not specified or None, an audio buffer is returned</li>
</ul>
</li>
<li>
<p><code>duration</code>:</p>
<ul>
<li>Type: int</li>
<li>What: The duration of time to record in seconds</li>
<li>Default: 3</li>
</ul>
</li>
</ul>
<h2 id="returns">Returns</h2>
<ul>
<li>
<p><code>audio_file</code>:</p>
<ul>
<li>Type: str</li>
<li>What: The provided <code>output_audio_file</code> given at the invokation of this function</li>
<li>Note: Returned only if <code>output_audio_file</code> is specified</li>
</ul>
</li>
<li>
<p>OR</p>
</li>
<li>
<p><code>audio_buffer</code>:</p>
<ul>
<li>Type: np.array</li>
<li>What: A 16-bit, mono raw audio signal at the appropriate sample rate serialized as a numpy array</li>
<li>Note: Exactly matches the DeepSpeech Model</li>
<li>Note: Returned only if <code>output_audio_file</code> is not specified</li>
</ul>
</li>
</ul></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def record(self, output_audio_file=None, duration=3):
    &#34;&#34;&#34;
    Record an audio file for a set duration using SoX

    Optional:

        - `output_audio_file`:
            - Type: str
            - What: The location to output the collected recording
            - Default: None
            - Note: Must be a `.wav` file
            - Note: If specified, the audio file location is returned
            - Note: If not specified or None, an audio buffer is returned

        - `duration`:
            - Type: int
            - What: The duration of time to record in seconds
            - Default: 3

    Returns:

        - `audio_file`:
            - Type: str
            - What: The provided `output_audio_file` given at the invokation of this function
            - Note: Returned only if `output_audio_file` is specified

        - OR

        - `audio_buffer`:
            - Type: np.array
            - What: A 16-bit, mono raw audio signal at the appropriate sample rate serialized as a numpy array
            - Note: Exactly matches the DeepSpeech Model
            - Note: Returned only if `output_audio_file` is not specified

    &#34;&#34;&#34;
    if output_audio_file:
        sox_cmd = f&#39;sox -d --channels 1 --rate {self.desired_sample_rate} --no-dither {shlex.quote(output_audio_file)} trim 0 {duration}&#39;
        output=self.execute_cmd(sox_cmd)
        return output_audio_file
    else:
        sox_cmd = f&#39;sox -d --type raw --bits 16 --channels 1 --rate {self.desired_sample_rate} --encoding signed-integer --endian little --compression 0.0 --no-dither - trim 0 {duration}&#39;
        output = self.execute_cmd(sox_cmd)
        return np.frombuffer(output.stdout, np.int16)</code></pre>
</details>
</dd>
<dt id="spych.core.spych.stt"><code class="name flex">
<span>def <span class="ident">stt</span></span>(<span>self, audio_buffer=None, audio_file=None)</span>
</code></dt>
<dd>
<div class="desc"><p>Compute speech-to-text transcription for a provided audio file</p>
<h2 id="required">Required</h2>
<ul>
<li>
<p><code>audio_buffer</code>:</p>
<ul>
<li>Type: np.array</li>
<li>What: A 16-bit, mono raw audio signal at the appropriate sample rate serialized as a numpy array</li>
<li>Note: Exactly matches the DeepSpeech Model</li>
</ul>
</li>
<li>
<p>OR</p>
</li>
<li>
<p><code>audio_file</code>:</p>
<ul>
<li>Type: str</li>
<li>What: The location of your target audio file to transcribe</li>
</ul>
</li>
</ul>
<h2 id="returns">Returns</h2>
<ul>
<li><code>text</code>:<ul>
<li>Type: str</li>
<li>What: The transcribed text</li>
</ul>
</li>
</ul></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def stt(self, audio_buffer=None, audio_file=None, ):
    &#34;&#34;&#34;
    Compute speech-to-text transcription for a provided audio file

    Required:

        - `audio_buffer`:
            - Type: np.array
            - What: A 16-bit, mono raw audio signal at the appropriate sample rate serialized as a numpy array
            - Note: Exactly matches the DeepSpeech Model

        - OR

        - `audio_file`:
            - Type: str
            - What: The location of your target audio file to transcribe

    Returns:

        - `text`:
            - Type: str
            - What: The transcribed text
    &#34;&#34;&#34;
    if audio_file:
        audio_buffer = self.parse_audio(audio_file)
    if audio_buffer is None:
        self.exception(&#34;You must specify a valid audio_file or audio_buffer&#34;)
    return self.model.stt(audio_buffer)</code></pre>
</details>
</dd>
<dt id="spych.core.spych.stt_expanded"><code class="name flex">
<span>def <span class="ident">stt_expanded</span></span>(<span>self, audio_file=None, audio_buffer=None, num_candidates=1, **kwargs)</span>
</code></dt>
<dd>
<div class="desc"><p>Compute speech-to-text with extra data for N predicted candidates given an audio file</p>
<h2 id="required">Required</h2>
<ul>
<li>
<p><code>audio_buffer</code>:</p>
<ul>
<li>Type: np.array</li>
<li>What: A 16-bit, mono raw audio signal at the appropriate sample rate serialized as a numpy array</li>
<li>Note: Exactly matches the DeepSpeech Model</li>
</ul>
</li>
<li>
<p>OR</p>
</li>
<li>
<p><code>audio_file</code>:</p>
<ul>
<li>Type: str</li>
<li>What: The location of your target audio file to transcribe</li>
</ul>
</li>
</ul>
<h2 id="optional">Optional</h2>
<ul>
<li><code>num_candidates</code>:<ul>
<li>Type: int</li>
<li>What: The number of potential transcript candidates to return</li>
<li>Default: 1</li>
<li>Note: The most confident/likely result appears first</li>
</ul>
</li>
<li><code>return_text</code>:<ul>
<li>Type: bool</li>
<li>What: Flag to indicate if the predicted text should be returned</li>
<li>Default: True</li>
</ul>
</li>
<li><code>return_confidence</code>:<ul>
<li>Type: bool</li>
<li>What: Flag to indicate if the confidence level for this text should be returned</li>
<li>Default: False</li>
</ul>
</li>
<li><code>return_words</code>:<ul>
<li>Type: bool</li>
<li>What: Flag to indicate if a words list (from the predicted text) should be returned</li>
<li>Default: False</li>
</ul>
</li>
<li><code>return_word_timings</code>:<ul>
<li>Type: bool</li>
<li>What: Flag to indicate if the predicted timings (start, end and duration) for each word should be returned</li>
<li>Default: False</li>
</ul>
</li>
<li><code>return_meta</code>:<ul>
<li>Type: bool</li>
<li>What: Flag to indicate if the transcript metadata object from DeepSpeech should be returned</li>
<li>Default: False</li>
</ul>
</li>
</ul>
<h2 id="returns">Returns</h2>
<ul>
<li><code>transcriptions</code>:<ul>
<li>Type: list of dictionaries</li>
<li>What: A list of dictionaries with various transcription output data</li>
</ul>
</li>
</ul></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def stt_expanded(self, audio_file=None, audio_buffer=None, num_candidates=1, **kwargs):
    &#34;&#34;&#34;
    Compute speech-to-text with extra data for N predicted candidates given an audio file

    Required:

        - `audio_buffer`:
            - Type: np.array
            - What: A 16-bit, mono raw audio signal at the appropriate sample rate serialized as a numpy array
            - Note: Exactly matches the DeepSpeech Model

        - OR

        - `audio_file`:
            - Type: str
            - What: The location of your target audio file to transcribe

    Optional:

        - `num_candidates`:
            - Type: int
            - What: The number of potential transcript candidates to return
            - Default: 1
            - Note: The most confident/likely result appears first
        - `return_text`:
            - Type: bool
            - What: Flag to indicate if the predicted text should be returned
            - Default: True
        - `return_confidence`:
            - Type: bool
            - What: Flag to indicate if the confidence level for this text should be returned
            - Default: False
        - `return_words`:
            - Type: bool
            - What: Flag to indicate if a words list (from the predicted text) should be returned
            - Default: False
        - `return_word_timings`:
            - Type: bool
            - What: Flag to indicate if the predicted timings (start, end and duration) for each word should be returned
            - Default: False
        - `return_meta`:
            - Type: bool
            - What: Flag to indicate if the transcript metadata object from DeepSpeech should be returned
            - Default: False

    Returns:

        - `transcriptions`:
            - Type: list of dictionaries
            - What: A list of dictionaries with various transcription output data
    &#34;&#34;&#34;
    if audio_file:
        audio_buffer = self.parse_audio(audio_file)
    if audio_buffer is None:
        self.exception(&#34;You must specify a valid audio_file or audio_buffer&#34;)
    output_meta=self.model.sttWithMetadata(audio_buffer, num_candidates)
    return [self.get_transcript_dict(transcript, **kwargs) for transcript in output_meta.transcripts]</code></pre>
</details>
</dd>
<dt id="spych.core.spych.stt_list"><code class="name flex">
<span>def <span class="ident">stt_list</span></span>(<span>self, audio_file=None, audio_buffer=None, num_candidates=3)</span>
</code></dt>
<dd>
<div class="desc"><p>Compute speech-to-text with extra data for N predicted candidates given an audio file</p>
<h2 id="required">Required</h2>
<ul>
<li>
<p><code>audio_buffer</code>:</p>
<ul>
<li>Type: np.array</li>
<li>What: A 16-bit, mono raw audio signal at the appropriate sample rate serialized as a numpy array</li>
<li>Note: Exactly matches the DeepSpeech Model</li>
</ul>
</li>
<li>
<p>OR</p>
</li>
<li>
<p><code>audio_file</code>:</p>
<ul>
<li>Type: str</li>
<li>What: The location of your target audio file to transcribe</li>
</ul>
</li>
</ul>
<h2 id="optional">Optional</h2>
<ul>
<li><code>num_candidates</code>:<ul>
<li>Type: int</li>
<li>What: The number of potential transcript candidates to return</li>
<li>Default: 1</li>
<li>Note: The most confident/likely result appears first</li>
</ul>
</li>
</ul>
<h2 id="returns">Returns</h2>
<ul>
<li><code>transcriptions</code>:<ul>
<li>Type: list of strs</li>
<li>What: A list of potential transcriptions in decending order of confidence</li>
</ul>
</li>
</ul></div>
<details class="source">
<summary>
<span>Expand source code</span>
</summary>
<pre><code class="python">def stt_list(self, audio_file=None, audio_buffer=None, num_candidates=3):
    &#34;&#34;&#34;
    Compute speech-to-text with extra data for N predicted candidates given an audio file

    Required:

        - `audio_buffer`:
            - Type: np.array
            - What: A 16-bit, mono raw audio signal at the appropriate sample rate serialized as a numpy array
            - Note: Exactly matches the DeepSpeech Model

        - OR

        - `audio_file`:
            - Type: str
            - What: The location of your target audio file to transcribe

    Optional:

        - `num_candidates`:
            - Type: int
            - What: The number of potential transcript candidates to return
            - Default: 1
            - Note: The most confident/likely result appears first

    Returns:

        - `transcriptions`:
            - Type: list of strs
            - What: A list of potential transcriptions in decending order of confidence
    &#34;&#34;&#34;
    expanded_list=self.stt_expanded(audio_file=audio_file, audio_buffer=audio_buffer, num_candidates=num_candidates, return_text=True)
    return [i[&#39;text&#39;] for i in expanded_list]</code></pre>
</details>
</dd>
</dl>
<h3>Inherited members</h3>
<ul class="hlist">
<li><code><b><a title="spych.utils.error" href="utils.html#spych.utils.error">error</a></b></code>:
<ul class="hlist">
<li><code><a title="spych.utils.error.exception" href="utils.html#spych.utils.error.exception">exception</a></code></li>
<li><code><a title="spych.utils.error.vprint" href="utils.html#spych.utils.error.vprint">vprint</a></code></li>
<li><code><a title="spych.utils.error.warn" href="utils.html#spych.utils.error.warn">warn</a></code></li>
</ul>
</li>
</ul>
</dd>
</dl>
</section>
</article>
<nav id="sidebar">
<h1>Index</h1>
<div class="toc">
<ul></ul>
</div>
<ul id="index">
<li><h3>Super-module</h3>
<ul>
<li><code><a title="spych" href="index.html">spych</a></code></li>
</ul>
</li>
<li><h3><a href="#header-classes">Classes</a></h3>
<ul>
<li>
<h4><code><a title="spych.core.spych" href="#spych.core.spych">spych</a></code></h4>
<ul class="two-column">
<li><code><a title="spych.core.spych.execute_cmd" href="#spych.core.spych.execute_cmd">execute_cmd</a></code></li>
<li><code><a title="spych.core.spych.get_transcript_dict" href="#spych.core.spych.get_transcript_dict">get_transcript_dict</a></code></li>
<li><code><a title="spych.core.spych.get_word_timings" href="#spych.core.spych.get_word_timings">get_word_timings</a></code></li>
<li><code><a title="spych.core.spych.parse_audio" href="#spych.core.spych.parse_audio">parse_audio</a></code></li>
<li><code><a title="spych.core.spych.parse_audio_sox" href="#spych.core.spych.parse_audio_sox">parse_audio_sox</a></code></li>
<li><code><a title="spych.core.spych.play" href="#spych.core.spych.play">play</a></code></li>
<li><code><a title="spych.core.spych.record" href="#spych.core.spych.record">record</a></code></li>
<li><code><a title="spych.core.spych.stt" href="#spych.core.spych.stt">stt</a></code></li>
<li><code><a title="spych.core.spych.stt_expanded" href="#spych.core.spych.stt_expanded">stt_expanded</a></code></li>
<li><code><a title="spych.core.spych.stt_list" href="#spych.core.spych.stt_list">stt_list</a></code></li>
</ul>
</li>
</ul>
</li>
</ul>
</nav>
</main>
<footer id="footer">
<p>Generated by <a href="https://pdoc3.github.io/pdoc"><cite>pdoc</cite> 0.9.2</a>.</p>
</footer>
</body>
</html>